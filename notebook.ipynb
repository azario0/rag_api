{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Getting the document\n",
    "# 2 Extract text\n",
    "# 3 Split document into chuncks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Any\n",
    "from langchain.schema.runnable import RunnableLambda\n",
    "import tempfile\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "import requests  \n",
    "\n",
    "# Function to download PDFs from URLs\n",
    "def download_pdfs(urls: List[str]) -> List[str]:\n",
    "    pdf_paths = []\n",
    "    for url in urls:\n",
    "        try:\n",
    "            response = requests.get(url)\n",
    "            response.raise_for_status()\n",
    "            with tempfile.NamedTemporaryFile(delete=False, suffix='.pdf') as temp_file:\n",
    "                temp_file.write(response.content)\n",
    "                pdf_paths.append(temp_file.name)\n",
    "        except Exception as e:\n",
    "            print(f\"Error downloading {url}: {str(e)}\")\n",
    "    return pdf_paths\n",
    "\n",
    "# Step 1: Define extract_text_from_pdfs for a single PDF path\n",
    "def extract_text_from_pdfs(pdf_path: str) -> Dict[str, Any]:\n",
    "    loader = PyPDFLoader(pdf_path)\n",
    "    pages = loader.load()\n",
    "\n",
    "    # Combine pages into a single document\n",
    "    full_text = \"\\n\".join(page.page_content for page in pages)\n",
    "\n",
    "    return {\n",
    "        \"text\": full_text,\n",
    "        \"source\": pdf_path,\n",
    "        \"pages\": len(pages)\n",
    "    }\n",
    "\n",
    "# Step 2: Define split_pdf_to_chunks to split extracted text\n",
    "def split_pdf_to_chunks(extracted_text: str, chunk_size: int = 10000) -> List[str]:\n",
    "    # Split the extracted text into chunks of specified size\n",
    "    chunks = [extracted_text[i:i + chunk_size] for i in range(0, len(extracted_text), chunk_size)]\n",
    "    return chunks\n",
    "\n",
    "# Step 3: Define the RunnableLambda for each step\n",
    "extract_text = RunnableLambda(lambda pdf_path: extract_text_from_pdfs(pdf_path)[\"text\"])  # Extract the text\n",
    "chuncks_split = RunnableLambda(lambda extracted_text: split_pdf_to_chunks(extracted_text))  # Split the text into chunks\n",
    "\n",
    "# Step 4: Chain the runnables together\n",
    "split_pdf_runnable = extract_text | chuncks_split\n",
    "\n",
    "# Download the PDF\n",
    "# pdf_paths = download_pdfs(['https://arxiv.org/pdf/2410.15288'])  # This returns a list\n",
    "pdf_paths = download_pdfs(['https://arxiv.org/pdf/2410.17220'])\n",
    "\n",
    "# Pass the first PDF path to the pipeline\n",
    "if pdf_paths:\n",
    "    pdf_path = pdf_paths[0]  # Extract the single path from the list\n",
    "\n",
    "    # Invoke the runnable chain on the single PDF path\n",
    "    chunks = split_pdf_runnable.invoke(pdf_path)\n",
    "\n",
    "else:\n",
    "    print(\"No PDF files were downloaded.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Extracting informations from chuncks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.messages import HumanMessage\n",
    "from typing import List\n",
    "# Initialize the LLM\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"models/gemini-1.0-pro\",\n",
    "    temperature=0.3,\n",
    "    google_api_key=\"YOUR_API_KEY\"\n",
    ")\n",
    "# Step 1: Create a Prompt Template for Information Extraction\n",
    "extract_info_prompt = \"\"\"\n",
    "Analyze the following section of a scientific document and extract key information:\n",
    "1. Title (if present)\n",
    "2. Authors (if present)\n",
    "3. Main research questions/objectives\n",
    "4. Methodology details\n",
    "5. Key findings and results\n",
    "6. Conclusions or implications\n",
    "7. Important citations or references\n",
    "8. Technical terms and their definitions\n",
    "\n",
    "If certain information is not present in this section, focus on extracting what is available.\n",
    "\n",
    "Text: {text}\n",
    "\n",
    "Provide the information in a structured format.\n",
    "\"\"\"\n",
    "# Prompt Template for the LLM\n",
    "prompt_template = PromptTemplate(input_variables=[\"text\"], template=extract_info_prompt)\n",
    "# Create a function to invoke the LLM for each chunk\n",
    "def extract_information_from_chunk(chunk: str) -> str:\n",
    "    # Format the prompt\n",
    "    formatted_prompt = prompt_template.format(text=chunk)\n",
    "    # Create a message\n",
    "    messages = [HumanMessage(content=formatted_prompt)]\n",
    "    # Get the response\n",
    "    response = llm.invoke(messages)\n",
    "    # Return the response content\n",
    "    return response.content\n",
    "\n",
    "# Step 2: Turn the function into a RunnableLambda\n",
    "from langchain.schema.runnable import RunnableLambda\n",
    "extract_info_runnable = RunnableLambda(extract_information_from_chunk)\n",
    "# Step 3: Apply the LLM to each chunk\n",
    "def process_chunks(chunks: List[str]) -> List[str]:\n",
    "    return [extract_info_runnable.invoke(chunk) for chunk in chunks]\n",
    "# Use the function\n",
    "extracted_info = process_chunks(chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 Merging the chuncks proceeded <br> and saving them in JSON file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "def parse_chunk(text: str) -> Dict[str, Any]:\n",
    "    \"\"\"Parse a single chunk of text into a dictionary of paper information.\"\"\"\n",
    "    info_dict = {\n",
    "        \"Title\": None,\n",
    "        \"Authors\": None,\n",
    "        \"Research Questions\": None,\n",
    "        \"Methodology\": None,\n",
    "        \"Findings\": None,\n",
    "        \"Conclusions\": None,\n",
    "        \"Citations\": None,\n",
    "        \"Technical Terms\": None\n",
    "    }\n",
    "    \n",
    "    # Define mapping of possible section headers to standardized keys\n",
    "    header_mapping = {\n",
    "        \"title\": \"Title\",\n",
    "        \"authors\": \"Authors\",\n",
    "        \"main research questions/objectives\": \"Research Questions\",\n",
    "        \"methodology details\": \"Methodology\",\n",
    "        \"key findings and results\": \"Findings\",\n",
    "        \"conclusions or implications\": \"Conclusions\",\n",
    "        \"important citations or references\": \"Citations\",\n",
    "        \"technical terms and their definitions\": \"Technical Terms\"\n",
    "    }\n",
    "    \n",
    "    current_section = None\n",
    "    current_content = []\n",
    "    \n",
    "    # Split text into lines and process\n",
    "    lines = text.split('\\n')\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "            \n",
    "        # Remove bold markers and numbers at start\n",
    "        clean_line = re.sub(r'\\*\\*|\\d+\\.\\s*|\\d+\\:\\s*', '', line).strip()\n",
    "        lower_clean = clean_line.lower()\n",
    "        \n",
    "        # Check if this is a section header\n",
    "        found_header = False\n",
    "        for header_pattern, standard_key in header_mapping.items():\n",
    "            if lower_clean.startswith(header_pattern):\n",
    "                # Save previous section content if exists\n",
    "                if current_section and current_content:\n",
    "                    content_text = ' '.join(current_content).strip()\n",
    "                    if content_text:\n",
    "                        info_dict[current_section] = content_text\n",
    "                \n",
    "                # Start new section\n",
    "                current_section = standard_key\n",
    "                current_content = []\n",
    "                found_header = True\n",
    "                \n",
    "                # Get any content after the header\n",
    "                header_content = clean_line[len(header_pattern):].strip()\n",
    "                if header_content and header_content not in [':', '-']:\n",
    "                    current_content.append(header_content)\n",
    "                break\n",
    "        \n",
    "        # If not a header, append to current section content\n",
    "        if not found_header and current_section:\n",
    "            current_content.append(clean_line)\n",
    "    \n",
    "    # Save the last section\n",
    "    if current_section and current_content:\n",
    "        content_text = ' '.join(current_content).strip()\n",
    "        if content_text:\n",
    "            info_dict[current_section] = content_text\n",
    "    \n",
    "    return info_dict\n",
    "\n",
    "def consolidate_chunks(chunks: List[str]) -> Dict[str, Any]:\n",
    "    \"\"\"Consolidate all chunks into a single dictionary with combined content.\"\"\"\n",
    "    all_content = {\n",
    "        \"Title\": [],\n",
    "        \"Authors\": [],\n",
    "        \"Research Questions\": [],\n",
    "        \"Methodology\": [],\n",
    "        \"Findings\": [],\n",
    "        \"Conclusions\": [],\n",
    "        \"Citations\": [],\n",
    "        \"Technical Terms\": []\n",
    "    }\n",
    "    \n",
    "    # Process each chunk\n",
    "    for chunk in chunks:\n",
    "        if not chunk or not isinstance(chunk, (str, bytes)):\n",
    "            continue\n",
    "            \n",
    "        chunk_info = parse_chunk(str(chunk))\n",
    "        \n",
    "        # Add non-None values to respective lists\n",
    "        for key, value in chunk_info.items():\n",
    "            if value is not None:\n",
    "                all_content[key].append(value)\n",
    "    \n",
    "    # Combine content and create final output\n",
    "    final_output = {}\n",
    "    \n",
    "    for key, values in all_content.items():\n",
    "        if values:  # Only include non-empty lists\n",
    "            # Join all values with proper separation\n",
    "            if key == \"Authors\":\n",
    "                # Split authors by common separators and create unique list\n",
    "                author_list = []\n",
    "                for author_text in values:\n",
    "                    authors = re.split(r'[,;]\\s*|\\s+and\\s+|\\s*\\|\\s*', author_text)\n",
    "                    author_list.extend([a.strip() for a in authors if a.strip()])\n",
    "                final_output[key] = list(dict.fromkeys(author_list))\n",
    "            else:\n",
    "                # Combine other fields with proper separation\n",
    "                combined = \" | \".join(values)\n",
    "                if combined.strip():\n",
    "                    final_output[key] = combined\n",
    "    \n",
    "    return final_output\n",
    "\n",
    "def process_and_save_json(extracted_info: List[str], output_file: str = \"document_analysis2.json\") -> Dict[str, Any]:\n",
    "    \"\"\"Process extracted information and save as JSON.\"\"\"\n",
    "    # Consolidate all chunks\n",
    "    consolidated_data = consolidate_chunks(extracted_info)\n",
    "    \n",
    "    # Save to JSON file\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(consolidated_data, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    return consolidated_data\n",
    "\n",
    "result = process_and_save_json(extracted_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
